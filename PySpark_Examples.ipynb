{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14168416\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "'''\n",
    " The building block of the Spark API is its RDD API. In the RDD API, there are two types of operations: \n",
    " transformations, which define a new dataset based on previous ones, and actions, which kick off a job \n",
    " to execute on a cluster. On top of Spark’s RDD API, high level APIs are provided, e.g. DataFrame API and \n",
    " Machine Learning API. These high level APIs provide a concise way to conduct certain data operations. \n",
    "\n",
    "1> PI ESTIMATION\n",
    "This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) \n",
    "and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate.\n",
    "\n",
    "'''\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2> WORD COUNT\n",
    "In this example, we use a few transformations to build a dataset of (String, Int) pairs called counts and then save \n",
    "it to a file.\n",
    "'''\n",
    "sc = pyspark.SparkContext(appName=\"WordCount\")\n",
    "text_file = sc.textFile(\"UIMASummerSchool2003.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"Spark_Wordcount_output.txt\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-7.4926373274095...|[5.56861309905893...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[5.86563908503424...|[0.99717280516679...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-6.8135956206001...|[0.00109752926021...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MLlib, Spark’s Machine Learning (ML) library, provides many distributed ML algorithms. These algorithms cover tasks \n",
    "such as feature extraction, classification, regression, clustering, recommendation, and more. MLlib also provides \n",
    "tools such as ML Pipelines for building workflows, CrossValidator for tuning parameters, and model persistence for \n",
    "saving and loading models.\n",
    "\n",
    "3> PREDICTION WITH LOGISTIC REGRESSION\n",
    "In this example, we take a dataset of labels and feature vectors. We learn to predict the labels from feature vectors \n",
    "using the Logistic Regression algorithm.\n",
    "'''\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"LogisticRegression\")\n",
    "\n",
    "diabetes_train = load_diabetes()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Prepare test data\n",
    "# Every record of this DataFrame contains the label and\n",
    "# features represented by a vector.\n",
    "df = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Set parameters for the algorithm.\n",
    "# Here, we limit the number of iterations to 10.\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Fit the model to the data.\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "model.transform(df).show()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.03807590643342...|151.0|206.07345904776457|\n",
      "|[-0.0018820165277...| 75.0| 68.11130074493124|\n",
      "|[0.08529890629667...|141.0| 176.8404128347199|\n",
      "|[-0.0890629393522...|206.0|166.85823029221248|\n",
      "|[0.00538306037424...|135.0|128.45256889870683|\n",
      "|[-0.0926954778032...| 97.0|106.33594406619089|\n",
      "|[-0.0454724779400...|138.0| 73.98067035208025|\n",
      "|[0.06350367559056...| 63.0|118.92115653222103|\n",
      "|[0.04170844488444...|110.0|158.82436723778153|\n",
      "|[-0.0709002470971...|310.0| 213.5851645958205|\n",
      "|[-0.0963280162542...|101.0| 97.14573768173105|\n",
      "|[0.02717829108036...| 69.0| 95.26108457593739|\n",
      "|[0.01628067572730...|179.0|115.05862144632293|\n",
      "|[0.00538306037424...|185.0|164.62484148553483|\n",
      "|[0.04534098333546...|118.0|103.05032974805697|\n",
      "|[-0.0527375548420...|171.0|177.10280850293867|\n",
      "|[-0.0055145549788...|166.0|211.70553577361468|\n",
      "|[0.0707687524926,...|144.0|182.82798585223452|\n",
      "|[-0.0382074010379...| 97.0|147.97045924456606|\n",
      "|[-0.0273097856849...|168.0|123.98114508514756|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "MSE = 2859.6954229074618\n",
      "RMSE = 53.47612011830572\n",
      "R-squared = 0.06822819279431147\n",
      "MAE = 43.27652770450133\n",
      "Explained variance = 5929.884896910382\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4> Predicting Diabetes using LinearRegression from MLib (Machine Learning library from Spark) \n",
    "\n",
    "This Diabetes dataset downloaded from Sklearn has ten baseline variables, age, sex, body mass index, average blood \n",
    "pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the \n",
    "response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "A fasting blood sugar level less than 100 mg/dL (5.6 mmol/L) is normal. A fasting blood sugar level from 100 to \n",
    "125 mg/dL (5.6 to 6.9 mmol/L) is considered prediabetes. If it's 126 mg/dL (7 mmol/L) or higher on two separate \n",
    "tests, you have diabetes. Oral glucose tolerance test.\n",
    "'''\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Import and clean data. Pyspark uses its own type system and unfortunately it doesn't deal with numpy well. \n",
    "# It works with python types though. So you need to manually convert the numpy.float64 to float.\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_features= []\n",
    "\n",
    "# Spark uses breeze under the hood for high performance Linear Algebra in Scala. In Spark, MLlib and other \n",
    "# ML algorithms depends on org.apache.spark.mllib.libalg.Vector type which is rather dense or sparse.\n",
    "\n",
    "for feature_list in diabetes.data:\n",
    "    temp= [float(i) for i in feature_list]\n",
    "    diabetes_features.append(Vectors.dense(temp))\n",
    "    \n",
    "diabetes_target = [float(i) for i in diabetes.target]\n",
    "features_and_predictions = list(zip(diabetes_target, diabetes_features))\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"LinearRegression_Diabetes\")\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(features_and_predictions, [\"label\", \"features\"])\n",
    "\n",
    "# Only max iterations is set. We will set parameters for the algorithm after ParamGridSearch\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "LR_model = tvs.fit(df)\n",
    "\n",
    "# Make predictions on test data. LR_model is the model with combination\n",
    "# of parameters that performed best.\n",
    "\n",
    "LR_model.transform(df)\\\n",
    "    .select(\"features\",\"label\", \"prediction\").show()\n",
    "\n",
    "Dataframe = LR_model.transform(df)\\\n",
    "    .select(\"label\", \"prediction\")\n",
    "\n",
    "# Metrics object needs to have an RDD of (prediction, observation) pairs.\n",
    "# Convert the dataframe object to an RDD\n",
    "\n",
    "valuesAndPreds = Dataframe.rdd.map(tuple)\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "\n",
    "# Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "\n",
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainValidationSplitModel_3b046625878d\n"
     ]
    }
   ],
   "source": [
    "print(LR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainValidationSplitModel' object has no attribute 'metricName'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2729778d567a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLR_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainValidationSplitModel' object has no attribute 'metricName'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Metrics object needs to have an RDD of (prediction, observation) pairs.\n",
    "\n",
    "\n",
    "valuesAndPreds = \n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "\n",
    "# Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "\n",
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n",
      "442\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.data)\n",
    "print(len(temporary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!! COPY !!!!\n",
    "\n",
    "4> Predicting Diabetes using LinearRegression from MLib (Machine Learning library from Spark) \n",
    "\n",
    "This Diabetes dataset downloaded from Sklearn has ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were \n",
    "obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease \n",
    "progression one year after baseline.\n",
    "\n",
    "A fasting blood sugar level less than 100 mg/dL (5.6 mmol/L) is normal. A fasting blood sugar level from 100 to \n",
    "125 mg/dL (5.6 to 6.9 mmol/L) is considered prediabetes. If it's 126 mg/dL (7 mmol/L) or higher on two separate \n",
    "tests, you have diabetes. Oral glucose tolerance test.\n",
    "'''\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import and clean data. Pyspark uses its own type system and unfortunately it doesn't deal with numpy well. \n",
    "# It works with python types though. So you need to manually convert the numpy.float64 to float.\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_data= []\n",
    "\n",
    "# Spark uses breeze under the hood for high performance Linear Algebra in Scala. In Spark, MLlib and other \n",
    "# ML algorithms depends on org.apache.spark.mllib.libalg.Vector type which is rather dense or sparse.\n",
    "\n",
    "for feature_list in diabetes.data[:]:\n",
    "    temp= [float(i) for i in feature_list]\n",
    "    diabetes_data.append(Vectors.dense(temp))\n",
    "    \n",
    "diabetes_target = [float(i) for i in diabetes.target]\n",
    "features_and_predictions = list(zip(diabetes_target, diabetes_train))\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"LinearRegression_Diabetes\")\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(features_and_predictions, [\"label\", \"features\"])\n",
    "\n",
    "# Only max iterations is set. We will set parameters for the algorithm after ParamGridSearch\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "\n",
    "# Fit the model to the data.\n",
    "lrModel = lr.fit(df)\n",
    "\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
